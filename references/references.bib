@article{ji2023hallucination,
  title        = {Survey of hallucination in natural language generation},
  author       = {Ji, Ziwei and Lee, Nayeon and Frieske, Rita and Yu, Tiezheng and Su, Dan and Xu, Yanrui and Ishii, Eric and Bang, Yejin and Madotto, Andrea and Fung, Pascale},
  journal      = {ACM Computing Surveys},
  year         = {2023},
  volume       = {55},
  number       = {12},
  pages        = {1--38},
  publisher    = {ACM},
  doi          = {10.1145/3571730}
}

@article{rao2023chatgpt,
  title        = {Assessing the factual accuracy of ChatGPT for medical information},
  author       = {Rao, Alex and Pang, Tiffany and Kim, Jenny and others},
  journal      = {JAMA Network Open},
  year         = {2023},
  volume       = {6},
  number       = {8},
  pages        = {e2325747},
  doi          = {10.1001/jamanetworkopen.2023.25747}
}

@article{kung2023usmle,
  title        = {Performance of ChatGPT on USMLE: Potential for AI-assisted medical education},
  author       = {Kung, Tiffany H and Cheatham, Michelle and Medenilla, Andrew and others},
  journal      = {PLOS Digital Health},
  year         = {2023},
  volume       = {2},
  number       = {2},
  pages        = {e0000198},
  doi          = {10.1371/journal.pdig.0000198}
}

@article{kadavath2022know,
  title        = {Language models (mostly) know what they know},
  author       = {Kadavath, Saurav and Conerly, Timothy and Ramesh, Aditya and others},
  journal      = {arXiv},
  year         = {2022},
  eprint       = {2207.05221},
  eprinttype   = {arXiv},
  url          = {https://arxiv.org/abs/2207.05221}
}

@inproceedings{si2023calibration,
  title        = {Measuring and improving calibration of large language models},
  author       = {Si, Shiyang and Wang, Ruixiang and Yu, Yixiao and others},
  booktitle    = {Advances in Neural Information Processing Systems},
  year         = {2023},
  url          = {https://openreview.net/forum?id=tgRkbY1iM5}
}

@article{meng2024llm_medical,
  title        = {The application of large language models in medicine},
  author       = {Meng, X and others},
  journal      = {Journal of Biomedical Informatics},
  year         = {2024}
}

@article{he2021deberta,
  title        = {DeBERTa: Decoding-enhanced BERT with disentangled attention},
  author       = {He, Pengcheng and Gao, Jianfeng and Chen, Weizhu},
  journal      = {arXiv},
  year         = {2021},
  eprint       = {2006.03654},
  eprinttype   = {arXiv},
  url          = {https://arxiv.org/abs/2006.03654}
}

@article{lee2025llm_clinical_eval,
  title        = {Clinical evaluation of large language models across accuracy, hallucinations, specificity, empathy, and actionability},
  author       = {Lee, Paul and others},
  journal      = {npj Digital Medicine},
  year         = {2025},
  volume       = {8},
  number       = {1},
  pages        = {18},
  doi          = {10.1038/s41746-025-01830-9},
  url          = {https://www.nature.com/articles/s41746-025-01830-9}
}

@online{moritzlaurer2022nli,
  author       = {Laurer, Moritz},
  title        = {DeBERTa-v3-large-mnli-fever-anli-ling-wanli},
  year         = {2022},
  organization = {Hugging Face},
  url          = {https://huggingface.co/MoritzLaurer/DeBERTa-v3-large-mnli-fever-anli-ling-wanli},
  urldate      = {2025-08-23}
}

@online{afroz_medquad_kaggle,
  author       = {Afroz},
  title        = {MedQuAD: Medical Question--Answer Dataset},
  year         = {2024},
  organization = {Kaggle},
  url          = {https://www.kaggle.com/datasets/pythonafroz/medquad-medical-question-answer-for-ai-research},
  urldate      = {2025-08-23}
}
